{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3620198,"sourceType":"datasetVersion","datasetId":2169207}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Step 1: Problem Definition and Data Understanding**","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\n\n# Load the dataset\ndata = pd.read_csv('Walmart_Store_sales.csv')  # Replace with the actual file path\n\n# Inspect the columns, data types, and initial statistics\nprint(data.info())\nprint(data.describe())\n\n# Focus on key features\nprint(data[['Weekly_Sales', 'Holiday_Flag', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']].head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 2: Data Cleaning and Preprocessing**","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\n\n# Check for stationarity using Augmented Dickey-Fuller test\ndef check_stationarity(series):\n    result = adfuller(series.dropna())\n    print(f'ADF Statistic: {result[0]}')\n    print(f'p-value: {result[1]}')\n    if result[1] > 0.05:\n        print(\"Series is non-stationary\")\n    else:\n        print(\"Series is stationary\")\n\ncheck_stationarity(data['Weekly_Sales'])\n\n# Apply transformations if non-stationary\ndata['Weekly_Sales_diff'] = data['Weekly_Sales'].diff().dropna()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Feature Engineering","metadata":{}},{"cell_type":"code","source":"# Create new time-based features\ndata['Date'] = pd.to_datetime(data['Date'])\ndata['WeekOfYear'] = data['Date'].dt.isocalendar().week\ndata['Month'] = data['Date'].dt.month\ndata['Season'] = data['Month'].apply(lambda x: 'Winter' if x in [12, 1, 2] else ('Spring' if x in [3, 4, 5] else ('Summer' if x in [6, 7, 8] else 'Fall')))\n\n# Expand Holiday_Flag to specific holiday flags (example shown with placeholders)\ndata['Super_Bowl'] = data['Holiday_Flag'] & (data['Date'].isin(['YYYY-MM-DD']))  # Replace with actual Super Bowl dates\ndata['Labor_Day'] = data['Holiday_Flag'] & (data['Date'].isin(['YYYY-MM-DD']))  # Replace with actual Labor Day dates\n\n# Generate rolling averages and lagged features for Weekly_Sales\ndata['Weekly_Sales_Rolling'] = data['Weekly_Sales'].rolling(window=4).mean()\ndata['Weekly_Sales_Lag1'] = data['Weekly_Sales'].shift(1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Encoding Categorical Features","metadata":{}},{"cell_type":"code","source":"# Encode categorical features\ndata = pd.get_dummies(data, columns=['Holiday_Flag', 'Season'], drop_first=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Scaling Continuous Features","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Scale continuous features\nscaler = StandardScaler()\ndata[['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']] = scaler.fit_transform(data[['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 3: Exploratory Data Analysis (EDA)**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# Seasonality and trend analysis\ndata.set_index('Date', inplace=True)\ndecomposition = seasonal_decompose(data['Weekly_Sales'], model='additive', period=52)\ndecomposition.plot()\nplt.show()\n\n# Holiday impact analysis\nsns.boxplot(x='Holiday_Flag', y='Weekly_Sales', data=data)\nplt.title(\"Impact of Holidays on Weekly Sales\")\nplt.show()\n\n# Correlation analysis\ncorr = data.corr()\nsns.heatmap(corr, annot=True, cmap='coolwarm')\nplt.title(\"Correlation Matrix\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 4: Data Preparation for Modeling**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Train-test split\ntrain_data = data[data['Date'] < 'YYYY-MM-DD']  # Define split date\ntest_data = data[data['Date'] >= 'YYYY-MM-DD']  # Define split date\n\n# Handling Non-Stationarity\ntrain_data['Weekly_Sales_diff'] = train_data['Weekly_Sales'].diff().dropna()\ntest_data['Weekly_Sales_diff'] = test_data['Weekly_Sales'].diff().dropna()\n\n# Feature selection based on feature importance from initial model\nfeatures = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'WeekOfYear', 'Super_Bowl', 'Labor_Day', 'Weekly_Sales_Lag1', 'Weekly_Sales_Rolling']\ntarget = 'Weekly_Sales'\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 5: Model Selection and Training**\n\nRandom Forest Regressor","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Train Random Forest model\nrf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\nrf_model.fit(train_data[features], train_data[target])\n\n# Predict and calculate error\npredictions = rf_model.predict(test_data[features])\nprint(\"MAE:\", mean_absolute_error(test_data[target], predictions))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ARIMA/SARIMA","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.arima.model import ARIMA\n\n# Train ARIMA model\narima_model = ARIMA(train_data['Weekly_Sales_diff'].dropna(), order=(1, 1, 1))\narima_results = arima_model.fit()\nprint(arima_results.summary())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 6: Model Evaluation**\nWeighted Mean Absolute Error (WMAE)","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Calculate WMAE\ndef weighted_mae(y_true, y_pred, weights):\n    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n\nholiday_weeks = test_data['Holiday_Flag'] == 1\nweights = np.where(holiday_weeks, 5, 1)  # Assign higher weights to holiday weeks\nwmae = weighted_mae(test_data[target], predictions, weights)\nprint(\"WMAE:\", wmae)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Additional Evaluation Metrics","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\n# Calculate RMSE\nrmse = np.sqrt(mean_squared_error(test_data[target], predictions))\nprint(\"RMSE:\", rmse)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Step 7: Insights and Business Recommendations**\n\nUsing your findings, provide actionable recommendations to Walmart based on the results:","metadata":{}},{"cell_type":"code","source":"# Insights Summary\nprint(\"Holiday weeks have significantly higher sales, with strong seasonality around major holidays.\")\nprint(\"Consider stocking more inventory and increasing promotions around Super Bowl and Labor Day.\")\nprint(\"Accurate demand predictions can reduce stockouts, optimize inventory, and boost revenue during peak periods.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}